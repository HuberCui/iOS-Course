# 基础知识
* 单词边界界定
    * 中文缺少词和词之间的界限符（中文自动分词）
* 上下文关联
* 基于统计学习的分词工具优于人工规则的分词工具。
* 未登录词造成的分词精度下降至少比分词歧义大 5 倍。
* 字标注统计学习方法能够大幅提高未登录词的识别率。


## 分词是最重要的
* 开源的中文分词工具，大多使用的也是开源的语料库，缺少大量的专有词汇。
* 貌似 HanLP 不是基于语料库的实现。


## 基础技术
* 词法分析：分词、词性标注、实体识别
* 词向量表示：语义挖掘、词义相似度
* 文本相似度：计算两个文本语义相似度，实现推荐和排序
* 依存句法分析：自动分析文本中的依存语法结构信息
* DNN（深度神经网络）语言模型：判断一句话是否符合语言表达习惯


## 应用技术
* 文本纠错
* 情感趋向分析
* 评论观点抽取
* 对话情绪识别
* 文本标签
* 文章分类
* 新闻摘要
* 文本审核
* 文本翻译


## 涉及到的知识点
* 统计学：最大概率、凝固度
* 信息论：信息熵、交叉熵
* 图模型：有向无环图、HMM 模型、CRF 模型
* 数据结构：Trie树、图结构
* 线性代数：余弦相似度
* 深度学习：神经网络、词向量训练及使用


## 什么是词？
最小的能够独立运用的语言单位，能单说有意义或用来造句的最小单位。


## 先分词后理解


## 中文分词歧义
鉴别一个人是否真的做过 NLP 就问这个

* 交集性歧义
    * ABC。可以是 AB/C，也可以是 A/BC
* 组合型歧义
    * AB。可以是 AB，也可以是 A/B


## HMM（隐马尔可夫模型）
统计模型，用来描述一个含有未知参数的马尔可夫过程。


## CRF（条件随机场）
给定一组输入序列条件下，另一组输出序列的条件概率分布模型



# 解决中文分词歧义

## 最大匹配法
是一种贪心算法，很可能错过更优的切分路径。因为贪心算法在每一步选择中都采取在当前情况下最好的选择，可能会陷入局部最优解的情况。


* 正向最大匹配法
* 逆向最大匹配法
* 双向最大匹配法。通过对比正向和逆向的匹配结果，如果结果不同，需要一个评估函数来决定最优的切分路径。

特殊情况：自建《不能单独成词的字表》



## 词总数最少法
我们为词总数最少法，添加上“不成词字表”的规则，每出现一个不成词的单字，就加罚一分。上面的例子就会得到如下的结果：

* “为人 / 民 / 服务”的罚分：1 + 2 + 1 = 4
* “为 / 人民 / 服务”的罚分：1 + 1 + 1 = 3

词总数最少法，切分路径结果里有多少词，就罚多少分，每出现一个不成词的单字，就加罚一分，罚分最少的就是最优的分词结果。

具体细节可看：
https://juejin.im/book/5d9ea8fff265da5b81794756/section/5da3e576f265da5b576be5a5



## 动态规划
https://juejin.im/post/5a29d52cf265da43333e4da7



## 语素
语言中最小的音义结合体，一个语言单位必须同时满足三个条件，「最小、有音、有义」才能被称做语素。


# N-Gram 切词法

一种基于统计语言模型的算法。将文本里的内容按照字节进行大小为 N 的滑动窗口切分，形成长度为 N 的字节片段序列。

* 泛化能力：一个机器学习算法对于没有见过的样本的识别能力。


## 马尔可夫假设
每隔词出现的概率只跟它前面的少数几个词有关。如一阶马尔可夫假设，只考虑前面一个词。


## N-Gram 模型在中文分词中的作用
当一段文本根据词典切分存在多种可能的结果时，如何选择最优的切分路径，可以使用 N-Gram 模型利用统计信息找出一条概率最大的路径，得到最终的分分词结果。


## 有向无环图（DAG）
在图论中，如果一个有向图从任意顶点出发无法经过若干条边回到该顶点。


## 归一化
目的将不同尺度上的评判结果统一到一个尺度上，从而可以做比较。例如，A 当月薪水为一头牛十斤麦子，B 当月薪水为一头羊一百斤玉米，很难比较到底谁的薪水更高，通过货币来进行归一化计算。

￼



